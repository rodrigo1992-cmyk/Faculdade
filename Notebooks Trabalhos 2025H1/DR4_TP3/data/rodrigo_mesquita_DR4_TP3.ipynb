{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a37131ce-d9cb-4b6d-8c2c-8982ac8b8b3c",
   "metadata": {},
   "source": [
    "# **Rodrigo_Mesquita_DR4_TP3**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13a3c8c",
   "metadata": {},
   "source": [
    "# üìÇ PARTE 1 -  An√°lise Est√°tica (Spark SQL + PostgreSQL + S3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f33cec",
   "metadata": {},
   "source": [
    "### 1. Realize a leitura da tabela apostas do PostgreSQL e transforme a coluna timestamp corretamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ca343c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "try:\n",
    "    spark.stop()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "properties = {\"user\": \"admin\",\"password\": \"admin\",\"driver\": \"org.postgresql.Driver\"}\n",
    "spark = (\n",
    "    SparkSession\n",
    "        .builder\n",
    "        .appName(\"TP4\")\n",
    "        .master(\"spark://spark:7077\")\n",
    "        .config(\"spark.jars.packages\", \"org.postgresql:postgresql:42.2.24,com.amazonaws:aws-java-sdk-bundle:1.12.262,org.apache.hadoop:hadoop-aws:3.3.4\")\n",
    "        .config('spark.hadoop.fs.s3a.endpoint', 'minio:9000')\n",
    "        .config('spark.hadoop.fs.s3a.access.key', 'admin')\n",
    "        .config('spark.hadoop.fs.s3a.secret.key', 'admin123')\n",
    "        .config('spark.hadoop.fs.s3a.path.style.access', 'true')\n",
    "        .config('spark.hadoop.fs.s3a.connection.ssl.enabled', 'false')\n",
    "        .config('spark.hadoop.fs.s3a.impl', 'org.apache.hadoop.fs.s3a.S3AFileSystem')\n",
    "        .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "15875843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- aposta_id: string (nullable = true)\n",
      " |-- apostador_id: string (nullable = true)\n",
      " |-- jogo_id: string (nullable = true)\n",
      " |-- valor_aposta: decimal(23,2) (nullable = true)\n",
      " |-- odd: decimal(23,2) (nullable = true)\n",
      " |-- hora_aposta: timestamp (nullable = true)\n",
      " |-- chave: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_apostas = spark.read.jdbc(\"jdbc:postgresql://postgres:5432/betalert\"  , \"apostas\", properties=properties)\n",
    "df_apostas.createOrReplaceTempView(\"apostas\")\n",
    "\n",
    "df_apostas = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "    aposta_id,\n",
    "    apostador_id,\n",
    "    jogo_id,\n",
    "    ROUND(valor, 2) AS valor_aposta,\n",
    "    ROUND(odd, 2) AS odd,\n",
    "    timestamp AS hora_aposta,\n",
    "    concat(date_format(timestamp, 'yyyy-MM-dd'), '_', apostador_id) AS chave\n",
    "    FROM apostas\n",
    "\"\"\")\n",
    "df_apostas.createOrReplaceTempView(\"apostas\")\n",
    "\n",
    "df_apostas.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8671e8b6-88ce-40a6-88f6-bfae1a592872",
   "metadata": {},
   "source": [
    "### 2. Realize a leitura da tabela transacoes_financeiras e normalize o nome da coluna de valor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "c9675f0e-451e-49e5-808e-0b86947a9f83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+---------------+--------+-------------------+--------------+\n",
      "| id|apostador_id|valor_transacao|    tipo|      hora_deposito|         chave|\n",
      "+---+------------+---------------+--------+-------------------+--------------+\n",
      "|  1|         u69|       14890.81|deposito|2025-01-01 10:00:00|2025-01-01_u69|\n",
      "|  2|         u94|        5616.48|deposito|2025-01-01 10:01:00|2025-01-01_u94|\n",
      "|  3|         u95|       16376.42|   saque|2025-01-01 10:02:00|2025-01-01_u95|\n",
      "|  4|         u11|       15335.80|deposito|2025-01-01 10:03:00|2025-01-01_u11|\n",
      "|  5|         u88|       19559.30|   saque|2025-01-01 10:04:00|2025-01-01_u88|\n",
      "|  6|         u32|        4797.95|deposito|2025-01-01 10:05:00|2025-01-01_u32|\n",
      "|  7|         u14|        1494.86|   saque|2025-01-01 10:06:00|2025-01-01_u14|\n",
      "|  8|         u23|       15609.39|deposito|2025-01-01 10:07:00|2025-01-01_u23|\n",
      "|  9|          u8|       14916.72|deposito|2025-01-01 10:08:00| 2025-01-01_u8|\n",
      "| 10|         u92|       12104.17|   saque|2025-01-01 10:09:00|2025-01-01_u92|\n",
      "| 11|         u72|        6943.49|   saque|2025-01-01 10:10:00|2025-01-01_u72|\n",
      "| 12|         u70|        7275.92|   saque|2025-01-01 10:11:00|2025-01-01_u70|\n",
      "| 13|         u75|         600.90|   saque|2025-01-01 10:12:00|2025-01-01_u75|\n",
      "| 14|         u45|       11640.29|deposito|2025-01-01 10:13:00|2025-01-01_u45|\n",
      "| 15|         u85|       16142.93|   saque|2025-01-01 10:14:00|2025-01-01_u85|\n",
      "| 16|          u1|        2532.39|   saque|2025-01-01 10:15:00| 2025-01-01_u1|\n",
      "| 17|         u68|       10645.14|deposito|2025-01-01 10:16:00|2025-01-01_u68|\n",
      "| 18|         u88|       11327.00|   saque|2025-01-01 10:17:00|2025-01-01_u88|\n",
      "| 19|         u36|        3473.10|   saque|2025-01-01 10:18:00|2025-01-01_u36|\n",
      "| 20|         u98|        4386.27|deposito|2025-01-01 10:19:00|2025-01-01_u98|\n",
      "+---+------------+---------------+--------+-------------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_transacoes = spark.read.jdbc(\"jdbc:postgresql://postgres:5432/betalert\"  , \"transacoes_financeiras\", properties=properties)\n",
    "df_transacoes.createOrReplaceTempView(\"transacoes\")\n",
    "\n",
    "\n",
    "df_transacoes = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "    id,\n",
    "    apostador_id,\n",
    "    ROUND(valor, 2) AS valor_transacao,\n",
    "    tipo,\n",
    "    data AS hora_deposito,\n",
    "    concat(date_format(data, 'yyyy-MM-dd'), '_', apostador_id) AS chave\n",
    "    FROM transacoes\n",
    "\"\"\")\n",
    "\n",
    "df_transacoes.createOrReplaceTempView(\"transacoes\")\n",
    "df_transacoes.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7769150b-5e76-4015-8843-5451dd2d8589",
   "metadata": {},
   "source": [
    "### 3. Fa√ßa o join entre apostas e o arquivo apostadores.csv do S3 para incluir o pa√≠s e dados extras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "ab579cac-5684-4c46-b7aa-7c9fc02aed98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+-------+------------+----+-------------------+---------------+----+-----------+--------------------+----+\n",
      "|aposta_id|apostador_id|jogo_id|valor_aposta| odd|        hora_aposta|          chave|  id|       nome|               email|pais|\n",
      "+---------+------------+-------+------------+----+-------------------+---------------+----+-----------+--------------------+----+\n",
      "| b61f4f08|         u79| jogo31|     1813.57|4.49|2025-01-05 15:42:00| 2025-01-05_u79| u79| Jogador 79|jogador79@exemplo...|  AR|\n",
      "| 88c44f52|         u23| jogo33|      830.60|2.87|2025-01-05 09:54:00| 2025-01-05_u23| u23| Jogador 23|jogador23@exemplo...|  FR|\n",
      "| 4c32051c|         u72| jogo72|     1829.92|2.09|2025-01-03 23:34:00| 2025-01-03_u72| u72| Jogador 72|jogador72@exemplo...|  ES|\n",
      "| 1baa492c|         u70| jogo67|     1614.20|4.21|2025-01-04 16:50:00| 2025-01-04_u70| u70| Jogador 70|jogador70@exemplo...|  ES|\n",
      "| 2820b7bb|         u40| jogo50|      989.73|2.98|2025-01-02 20:37:00| 2025-01-02_u40| u40| Jogador 40|jogador40@exemplo...|  FR|\n",
      "| 9f953950|         u10| jogo66|     1786.55|3.30|2025-01-04 13:58:00| 2025-01-04_u10| u10| Jogador 10|jogador10@exemplo...|  FR|\n",
      "| 1549ba26|         u95| jogo42|      683.48|3.22|2025-01-08 01:18:00| 2025-01-08_u95| u95| Jogador 95|jogador95@exemplo...|  ES|\n",
      "| 029f146f|         u71|  jogo2|     1239.37|4.33|2025-01-06 06:19:00| 2025-01-06_u71| u71| Jogador 71|jogador71@exemplo...|  ES|\n",
      "| 4b389fb4|         u51| jogo52|     1488.51|2.06|2025-01-02 00:53:00| 2025-01-02_u51| u51| Jogador 51|jogador51@exemplo...|  BR|\n",
      "| 4171ddaf|         u46| jogo52|      401.41|4.62|2025-01-06 04:19:00| 2025-01-06_u46| u46| Jogador 46|jogador46@exemplo...|  ES|\n",
      "| 4f489876|        u100|  jogo7|      333.46|4.03|2025-01-02 10:45:00|2025-01-02_u100|u100|Jogador 100|jogador100@exempl...|  BR|\n",
      "| a6bcf46f|         u89| jogo33|      840.46|1.67|2025-01-03 19:56:00| 2025-01-03_u89| u89| Jogador 89|jogador89@exemplo...|  FR|\n",
      "| afec5963|         u40| jogo25|      825.67|2.22|2025-01-05 12:08:00| 2025-01-05_u40| u40| Jogador 40|jogador40@exemplo...|  FR|\n",
      "| f6a6c13a|         u53| jogo90|      252.87|4.68|2025-01-05 23:46:00| 2025-01-05_u53| u53| Jogador 53|jogador53@exemplo...|  FR|\n",
      "| 04baf2d3|         u81|  jogo9|      199.32|4.34|2025-01-05 15:22:00| 2025-01-05_u81| u81| Jogador 81|jogador81@exemplo...|  ES|\n",
      "| e5385e0a|         u55| jogo21|      658.78|3.57|2025-01-03 05:31:00| 2025-01-03_u55| u55| Jogador 55|jogador55@exemplo...|  BR|\n",
      "| 23225f6a|         u39| jogo92|     1232.60|2.09|2025-01-06 09:23:00| 2025-01-06_u39| u39| Jogador 39|jogador39@exemplo...|  AR|\n",
      "| 13e7c2e9|         u58| jogo61|     1134.62|4.05|2025-01-07 19:42:00| 2025-01-07_u58| u58| Jogador 58|jogador58@exemplo...|  BR|\n",
      "| 619b83bb|         u20| jogo48|      362.30|3.71|2025-01-04 18:25:00| 2025-01-04_u20| u20| Jogador 20|jogador20@exemplo...|  FR|\n",
      "| aefa2523|         u59| jogo83|     1719.52|1.79|2025-01-08 02:59:00| 2025-01-08_u59| u59| Jogador 59|jogador59@exemplo...|  FR|\n",
      "+---------+------------+-------+------------+----+-------------------+---------------+----+-----------+--------------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_apostadores = spark.read.csv('s3a://betalogs/apostadores.csv', header=True)\n",
    "df_apostadores.createOrReplaceTempView(\"apostadores\")\n",
    "\n",
    "apostas_full = spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM apostas a\n",
    "    LEFT JOIN apostadores b \n",
    "    ON a.apostador_id = b.id\n",
    "\"\"\")\n",
    "\n",
    "apostas_full.createOrReplaceTempView(\"apostas_full\")\n",
    "\n",
    "apostas_full.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b705fc2",
   "metadata": {},
   "source": [
    "# üîç PARTE 2 - Detec√ß√£o de Padr√µes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc8a22d-254a-4ef1-adf6-dc601d1d9ac1",
   "metadata": {},
   "source": [
    "### 1. Identifique apostas-rel√¢mpago, ou seja, apostas feitas at√© 10 segundos ap√≥s dep√≥sitos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "f9455915-67ab-4f8b-9ebf-4debdbe829d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+---------------+--------+-------------------+--------------+---------+------------+-------+------------+-----+-------------------+--------------+---+----------+--------------------+----+---------------------+\n",
      "| id|apostador_id|valor_transacao|    tipo|      hora_deposito|         chave|aposta_id|apostador_id|jogo_id|valor_aposta|  odd|        hora_aposta|         chave| id|      nome|               email|pais|diferenca_em_segundos|\n",
      "+---+------------+---------------+--------+-------------------+--------------+---------+------------+-------+------------+-----+-------------------+--------------+---+----------+--------------------+----+---------------------+\n",
      "|503|         u41|       15000.00|deposito|2025-01-11 20:02:00|2025-01-11_u41| 066f9055|         u41| jogo71|    15000.00|16.46|2025-01-11 20:02:09|2025-01-11_u41|u41|Jogador 41|jogador41@exemplo...|  ES|                    9|\n",
      "|502|         u33|       15000.00|deposito|2025-01-11 20:01:00|2025-01-11_u33| d6002cae|         u33| jogo95|    15000.00|14.60|2025-01-11 20:01:01|2025-01-11_u33|u33|Jogador 33|jogador33@exemplo...|  ES|                    1|\n",
      "|510|         u66|       15000.00|deposito|2025-01-11 20:09:00|2025-01-11_u66| 0c7defda|         u66|  jogo6|    15000.00|12.23|2025-01-11 20:09:09|2025-01-11_u66|u66|Jogador 66|jogador66@exemplo...|  AR|                    9|\n",
      "|173|         u19|        8595.74|deposito|2025-01-01 12:52:00|2025-01-01_u19| df00489c|         u19| jogo95|      574.71| 2.60|2025-01-01 12:52:00|2025-01-01_u19|u19|Jogador 19|jogador19@exemplo...|  BR|                    0|\n",
      "|508|         u99|       15000.00|deposito|2025-01-11 20:07:00|2025-01-11_u99| dbf3315b|         u99| jogo16|    15000.00|11.54|2025-01-11 20:07:03|2025-01-11_u99|u99|Jogador 99|jogador99@exemplo...|  AR|                    3|\n",
      "|506|         u88|       15000.00|deposito|2025-01-11 20:05:00|2025-01-11_u88| 59c22669|         u88| jogo78|    15000.00|17.90|2025-01-11 20:05:08|2025-01-11_u88|u88|Jogador 88|jogador88@exemplo...|  ES|                    8|\n",
      "|507|         u95|       15000.00|deposito|2025-01-11 20:06:00|2025-01-11_u95| df1619c3|         u95| jogo25|    15000.00|13.44|2025-01-11 20:06:09|2025-01-11_u95|u95|Jogador 95|jogador95@exemplo...|  ES|                    9|\n",
      "|509|         u11|       15000.00|deposito|2025-01-11 20:08:00|2025-01-11_u11| 727a0ad2|         u11| jogo39|    15000.00|21.33|2025-01-11 20:08:07|2025-01-11_u11|u11|Jogador 11|jogador11@exemplo...|  BR|                    7|\n",
      "|504|         u58|       15000.00|deposito|2025-01-11 20:03:00|2025-01-11_u58| 34c39892|         u58| jogo60|    15000.00|11.54|2025-01-11 20:03:09|2025-01-11_u58|u58|Jogador 58|jogador58@exemplo...|  BR|                    9|\n",
      "|501|         u17|       15000.00|deposito|2025-01-11 20:00:00|2025-01-11_u17| 5b0e994f|         u17|  jogo8|    15000.00|15.32|2025-01-11 20:00:02|2025-01-11_u17|u17|Jogador 17|jogador17@exemplo...|  FR|                    2|\n",
      "|505|         u73|       15000.00|deposito|2025-01-11 20:04:00|2025-01-11_u73| db280beb|         u73|  jogo3|    15000.00|24.54|2025-01-11 20:04:07|2025-01-11_u73|u73|Jogador 73|jogador73@exemplo...|  ES|                    7|\n",
      "+---+------------+---------------+--------+-------------------+--------------+---------+------------+-------+------------+-----+-------------------+--------------+---+----------+--------------------+----+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "depositos = spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM transacoes\n",
    "    WHERE tipo = \"deposito\"\n",
    "\"\"\")\n",
    "depositos.createOrReplaceTempView(\"depositos\")\n",
    "\n",
    "\n",
    "apostas_relampago = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        a.*, \n",
    "        b.*,\n",
    "        (unix_timestamp(b.hora_aposta) - unix_timestamp(a.hora_deposito)) AS diferenca_em_segundos\n",
    "    FROM depositos a\n",
    "    LEFT JOIN apostas_full b\n",
    "    ON a.chave = b.chave\n",
    "    WHERE (unix_timestamp(b.hora_aposta) - unix_timestamp(a.hora_deposito)) < 10 AND (unix_timestamp(b.hora_aposta) - unix_timestamp(a.hora_deposito)) >= 0\n",
    "\"\"\")\n",
    "\n",
    "apostas_relampago.createOrReplaceTempView(\"apostas_relampago\")\n",
    "\n",
    "apostas_relampago.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d037554-c06c-4026-a29a-1a6ceff5f359",
   "metadata": {},
   "source": [
    "### 2. Exiba apostas-rel√¢mpago com valor acima de R$500."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "cf1a6283-8a31-4d97-a3c2-cc6b52b086e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+---------------+--------+-------------------+--------------+---------+------------+-------+------------+-----+-------------------+--------------+---+----------+--------------------+----+---------------------+\n",
      "| id|apostador_id|valor_transacao|    tipo|      hora_deposito|         chave|aposta_id|apostador_id|jogo_id|valor_aposta|  odd|        hora_aposta|         chave| id|      nome|               email|pais|diferenca_em_segundos|\n",
      "+---+------------+---------------+--------+-------------------+--------------+---------+------------+-------+------------+-----+-------------------+--------------+---+----------+--------------------+----+---------------------+\n",
      "|173|         u19|        8595.74|deposito|2025-01-01 12:52:00|2025-01-01_u19| df00489c|         u19| jogo95|      574.71| 2.60|2025-01-01 12:52:00|2025-01-01_u19|u19|Jogador 19|jogador19@exemplo...|  BR|                    0|\n",
      "|501|         u17|       15000.00|deposito|2025-01-11 20:00:00|2025-01-11_u17| 5b0e994f|         u17|  jogo8|    15000.00|15.32|2025-01-11 20:00:02|2025-01-11_u17|u17|Jogador 17|jogador17@exemplo...|  FR|                    2|\n",
      "|502|         u33|       15000.00|deposito|2025-01-11 20:01:00|2025-01-11_u33| d6002cae|         u33| jogo95|    15000.00|14.60|2025-01-11 20:01:01|2025-01-11_u33|u33|Jogador 33|jogador33@exemplo...|  ES|                    1|\n",
      "|503|         u41|       15000.00|deposito|2025-01-11 20:02:00|2025-01-11_u41| 066f9055|         u41| jogo71|    15000.00|16.46|2025-01-11 20:02:09|2025-01-11_u41|u41|Jogador 41|jogador41@exemplo...|  ES|                    9|\n",
      "|504|         u58|       15000.00|deposito|2025-01-11 20:03:00|2025-01-11_u58| 34c39892|         u58| jogo60|    15000.00|11.54|2025-01-11 20:03:09|2025-01-11_u58|u58|Jogador 58|jogador58@exemplo...|  BR|                    9|\n",
      "|505|         u73|       15000.00|deposito|2025-01-11 20:04:00|2025-01-11_u73| db280beb|         u73|  jogo3|    15000.00|24.54|2025-01-11 20:04:07|2025-01-11_u73|u73|Jogador 73|jogador73@exemplo...|  ES|                    7|\n",
      "|506|         u88|       15000.00|deposito|2025-01-11 20:05:00|2025-01-11_u88| 59c22669|         u88| jogo78|    15000.00|17.90|2025-01-11 20:05:08|2025-01-11_u88|u88|Jogador 88|jogador88@exemplo...|  ES|                    8|\n",
      "|507|         u95|       15000.00|deposito|2025-01-11 20:06:00|2025-01-11_u95| df1619c3|         u95| jogo25|    15000.00|13.44|2025-01-11 20:06:09|2025-01-11_u95|u95|Jogador 95|jogador95@exemplo...|  ES|                    9|\n",
      "|508|         u99|       15000.00|deposito|2025-01-11 20:07:00|2025-01-11_u99| dbf3315b|         u99| jogo16|    15000.00|11.54|2025-01-11 20:07:03|2025-01-11_u99|u99|Jogador 99|jogador99@exemplo...|  AR|                    3|\n",
      "|509|         u11|       15000.00|deposito|2025-01-11 20:08:00|2025-01-11_u11| 727a0ad2|         u11| jogo39|    15000.00|21.33|2025-01-11 20:08:07|2025-01-11_u11|u11|Jogador 11|jogador11@exemplo...|  BR|                    7|\n",
      "|510|         u66|       15000.00|deposito|2025-01-11 20:09:00|2025-01-11_u66| 0c7defda|         u66|  jogo6|    15000.00|12.23|2025-01-11 20:09:09|2025-01-11_u66|u66|Jogador 66|jogador66@exemplo...|  AR|                    9|\n",
      "+---+------------+---------------+--------+-------------------+--------------+---------+------------+-------+------------+-----+-------------------+--------------+---+----------+--------------------+----+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "apostas_relampago_500 = spark.sql(\"\"\"\n",
    "    SELECT * FROM apostas_relampago\n",
    "    WHERE valor_aposta > 500\n",
    "\"\"\")\n",
    "\n",
    "apostas_relampago_500.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0d6e97-01fe-49eb-8674-7951aac9ef70",
   "metadata": {},
   "source": [
    "### 3. Exiba apostas-rel√¢mpago com valor acima de R$10.000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "28d1f497-55b5-49f1-8525-6191abe7259b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+---------------+--------+-------------------+--------------+---------+------------+-------+------------+-----+-------------------+--------------+---+----------+--------------------+----+---------------------+\n",
      "| id|apostador_id|valor_transacao|    tipo|      hora_deposito|         chave|aposta_id|apostador_id|jogo_id|valor_aposta|  odd|        hora_aposta|         chave| id|      nome|               email|pais|diferenca_em_segundos|\n",
      "+---+------------+---------------+--------+-------------------+--------------+---------+------------+-------+------------+-----+-------------------+--------------+---+----------+--------------------+----+---------------------+\n",
      "|501|         u17|       15000.00|deposito|2025-01-11 20:00:00|2025-01-11_u17| 5b0e994f|         u17|  jogo8|    15000.00|15.32|2025-01-11 20:00:02|2025-01-11_u17|u17|Jogador 17|jogador17@exemplo...|  FR|                    2|\n",
      "|502|         u33|       15000.00|deposito|2025-01-11 20:01:00|2025-01-11_u33| d6002cae|         u33| jogo95|    15000.00|14.60|2025-01-11 20:01:01|2025-01-11_u33|u33|Jogador 33|jogador33@exemplo...|  ES|                    1|\n",
      "|503|         u41|       15000.00|deposito|2025-01-11 20:02:00|2025-01-11_u41| 066f9055|         u41| jogo71|    15000.00|16.46|2025-01-11 20:02:09|2025-01-11_u41|u41|Jogador 41|jogador41@exemplo...|  ES|                    9|\n",
      "|504|         u58|       15000.00|deposito|2025-01-11 20:03:00|2025-01-11_u58| 34c39892|         u58| jogo60|    15000.00|11.54|2025-01-11 20:03:09|2025-01-11_u58|u58|Jogador 58|jogador58@exemplo...|  BR|                    9|\n",
      "|505|         u73|       15000.00|deposito|2025-01-11 20:04:00|2025-01-11_u73| db280beb|         u73|  jogo3|    15000.00|24.54|2025-01-11 20:04:07|2025-01-11_u73|u73|Jogador 73|jogador73@exemplo...|  ES|                    7|\n",
      "|506|         u88|       15000.00|deposito|2025-01-11 20:05:00|2025-01-11_u88| 59c22669|         u88| jogo78|    15000.00|17.90|2025-01-11 20:05:08|2025-01-11_u88|u88|Jogador 88|jogador88@exemplo...|  ES|                    8|\n",
      "|507|         u95|       15000.00|deposito|2025-01-11 20:06:00|2025-01-11_u95| df1619c3|         u95| jogo25|    15000.00|13.44|2025-01-11 20:06:09|2025-01-11_u95|u95|Jogador 95|jogador95@exemplo...|  ES|                    9|\n",
      "|508|         u99|       15000.00|deposito|2025-01-11 20:07:00|2025-01-11_u99| dbf3315b|         u99| jogo16|    15000.00|11.54|2025-01-11 20:07:03|2025-01-11_u99|u99|Jogador 99|jogador99@exemplo...|  AR|                    3|\n",
      "|509|         u11|       15000.00|deposito|2025-01-11 20:08:00|2025-01-11_u11| 727a0ad2|         u11| jogo39|    15000.00|21.33|2025-01-11 20:08:07|2025-01-11_u11|u11|Jogador 11|jogador11@exemplo...|  BR|                    7|\n",
      "|510|         u66|       15000.00|deposito|2025-01-11 20:09:00|2025-01-11_u66| 0c7defda|         u66|  jogo6|    15000.00|12.23|2025-01-11 20:09:09|2025-01-11_u66|u66|Jogador 66|jogador66@exemplo...|  AR|                    9|\n",
      "+---+------------+---------------+--------+-------------------+--------------+---------+------------+-------+------------+-----+-------------------+--------------+---+----------+--------------------+----+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "apostas_relampago_10k = spark.sql(\"\"\"\n",
    "    SELECT * FROM apostas_relampago\n",
    "    WHERE valor_aposta > 10000\n",
    "\"\"\")\n",
    "\n",
    "apostas_relampago_10k.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea966ac-c14a-405d-b6da-4f56fd0a273f",
   "metadata": {},
   "source": [
    "### 4. Detecte jogadores que realizaram 10 ou mais apostas em um mesmo jogo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "55717cb3-d17c-4fa2-9f06-49661c917611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+-----------+\n",
      "|jogo_id|apostador_id|qtd_apostas|\n",
      "+-------+------------+-----------+\n",
      "| jogo77|         u88|         13|\n",
      "+-------+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "multiplas_apostas = spark.sql(\"\"\"\n",
    "    SELECT jogo_id, apostador_id, COUNT(apostador_id) as qtd_apostas\n",
    "    FROM apostas\n",
    "    GROUP BY jogo_id, apostador_id\n",
    "    HAVING COUNT(apostador_id) >= 10\n",
    "\"\"\")\n",
    "\n",
    "multiplas_apostas.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5907cd",
   "metadata": {},
   "source": [
    "### 5. Exiba o total e a m√©dia de valores apostados por pa√≠s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "bdf98178-6b38-4971-bc3d-7dcdff6d6ea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------------+-------------------+\n",
      "|pais|valor_total_apostas|valor_medio_apostas|\n",
      "+----+-------------------+-------------------+\n",
      "|  PT|          627445.73|            1012.01|\n",
      "|  BR|         1129415.56|            1070.54|\n",
      "|  ES|         1323581.35|            1075.21|\n",
      "|  FR|         1265400.11|            1038.06|\n",
      "|  AR|          945906.52|            1066.41|\n",
      "+----+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "media_apostas = spark.sql(\"\"\"\n",
    "    SELECT pais,\n",
    "    ROUND(SUM(valor),2) AS valor_total_apostas,\n",
    "    ROUND(AVG(valor),2) AS valor_medio_apostas\n",
    "    FROM apostasXapostadores\n",
    "    GROUP BY pais\n",
    "\"\"\")\n",
    "\n",
    "media_apostas.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018d52f9",
   "metadata": {},
   "source": [
    "# üì° PARTE 3 - Streaming em Tempo Real (Kafka + Spark Structured Streaming)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349f6da5-e9c6-443f-83a0-6bf8186e4267",
   "metadata": {},
   "source": [
    "### 1. Consuma o t√≥pico Kafka stream_apostas e transforme os dados conforme o schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d33e2449-8edd-49bf-b568-137dd745d759",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, col, expr\n",
    "from pyspark.sql.types import *\n",
    "import time \n",
    "\n",
    "    \n",
    "try:\n",
    "    spark.stop()\n",
    "except:\n",
    "    pass\n",
    "    \n",
    "    \n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "        .appName(\"TP4\")\n",
    "        .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,org.postgresql:postgresql:42.2.24,com.amazonaws:aws-java-sdk-bundle:1.12.262,org.apache.hadoop:hadoop-aws:3.3.4\")\n",
    "        .config('spark.hadoop.fs.s3a.endpoint', 'minio:9000')\n",
    "        .config('spark.hadoop.fs.s3a.access.key', 'admin')\n",
    "        .config('spark.hadoop.fs.s3a.secret.key', 'admin123')\n",
    "        .config('spark.hadoop.fs.s3a.path.style.access', 'true')\n",
    "        .config('spark.hadoop.fs.s3a.connection.ssl.enabled', 'false')\n",
    "        .config('spark.hadoop.fs.s3a.impl', 'org.apache.hadoop.fs.s3a.S3AFileSystem')\n",
    "        .getOrCreate()\n",
    ")\n",
    "\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"aposta_id\", StringType()),\n",
    "    StructField(\"apostador_id\", StringType()),\n",
    "    StructField(\"jogo_id\", StringType()),\n",
    "    StructField(\"valor\", DoubleType()),    \n",
    "    StructField(\"odd\", FloatType()),        \n",
    "    StructField(\"timestamp\", TimestampType())\n",
    "])\n",
    "\n",
    "df_raw = (\n",
    "    spark.readStream\n",
    "        .format(\"kafka\")\n",
    "        .option(\"kafka.bootstrap.servers\", \"kafka1:9092\")\n",
    "        .option(\"subscribe\", \"stream_apostas\")\n",
    "        .load()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "43ea6fcc-579f-43fe-8b4e-3d1e97e26141",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_json = df_raw.selectExpr(\"CAST (value as STRING) as json\") \\\n",
    "        .select(from_json(col(\"json\"), schema).alias(\"data\")) \\\n",
    "        .select(\"data.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4f25256-ccc2-4c7c-8da2-9bfca7f77805",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/04 03:09:03 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-dcdac508-28a9-43ee-a7e1-b80e63230afc. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/06/04 03:09:03 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "25/06/04 03:09:03 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+---------+------------+-------+-----+---+---------+\n",
      "|aposta_id|apostador_id|jogo_id|valor|odd|timestamp|\n",
      "+---------+------------+-------+-----+---+---------+\n",
      "+---------+------------+-------+-----+---+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+---------+------------+-------+-------+----+--------------------+\n",
      "|aposta_id|apostador_id|jogo_id|  valor| odd|           timestamp|\n",
      "+---------+------------+-------+-------+----+--------------------+\n",
      "| c534f654|          u4| jogo67|3690.78|3.38|2025-06-04 03:09:...|\n",
      "| ea4d904d|         u89| jogo89| 3430.4|6.28|2025-06-04 03:09:...|\n",
      "| 29075068|         u38| jogo55|4669.07|6.87|2025-06-04 03:09:...|\n",
      "| df3a5fc5|         u49| jogo82|3897.22|3.57|2025-06-04 03:09:...|\n",
      "| b4837ba2|         u82| jogo98|4115.12|6.55|2025-06-04 03:09:...|\n",
      "| d43a9488|         u27| jogo94|4195.53|3.48|2025-06-04 03:09:...|\n",
      "| dfa666a1|         u97| jogo10|3776.34|5.83|2025-06-04 03:09:...|\n",
      "| 7eb0ccbb|         u80| jogo45|4089.89|8.07|2025-06-04 03:09:...|\n",
      "| 5579fafe|         u34| jogo87|4065.66|4.48|2025-06-04 03:09:...|\n",
      "+---------+------------+-------+-------+----+--------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+---------+------------+-------+-------+----+--------------------+\n",
      "|aposta_id|apostador_id|jogo_id|  valor| odd|           timestamp|\n",
      "+---------+------------+-------+-------+----+--------------------+\n",
      "| 41acfc8d|         u66| jogo63|3212.13|6.78|2025-06-04 03:09:...|\n",
      "| 464176bf|          u4| jogo11|1823.72|5.07|2025-06-04 03:09:...|\n",
      "| 42aba272|         u23| jogo71| 3090.4| 6.0|2025-06-04 03:09:...|\n",
      "| fcdff2ff|         u41| jogo79| 954.07|2.22|2025-06-04 03:09:...|\n",
      "| eae36ef3|         u51| jogo54|3268.16|1.29|2025-06-04 03:09:...|\n",
      "| e3dbd75d|         u99| jogo51|4554.47| 3.8|2025-06-04 03:09:...|\n",
      "| 83217a95|         u70| jogo43|1757.46|8.81|2025-06-04 03:09:...|\n",
      "| 8e5a2375|         u44| jogo96|2900.87|1.52|2025-06-04 03:09:...|\n",
      "| 2fa58be2|         u36| jogo52|3884.33|3.19|2025-06-04 03:09:...|\n",
      "| 7b838708|         u24| jogo60| 290.61|2.17|2025-06-04 03:09:...|\n",
      "| d2ebd2a3|          u8| jogo26|1670.23|1.96|2025-06-04 03:09:...|\n",
      "| 02cc8256|         u44| jogo75| 235.45|8.82|2025-06-04 03:09:...|\n",
      "| e4fc2ae2|         u10|  jogo1|2848.77|4.28|2025-06-04 03:09:...|\n",
      "| 04e201ed|         u12| jogo18|3451.66|2.21|2025-06-04 03:09:...|\n",
      "| 07628234|          u4| jogo22|3026.62|7.17|2025-06-04 03:09:...|\n",
      "| f5b6c510|          u7| jogo76|3077.99|2.97|2025-06-04 03:09:...|\n",
      "| 48dcdad4|          u7| jogo73|1692.84|4.55|2025-06-04 03:09:...|\n",
      "| 87b4b401|         u62| jogo89|1534.71|4.86|2025-06-04 03:09:...|\n",
      "| 5d6f345f|         u67| jogo99|3231.13|6.54|2025-06-04 03:09:...|\n",
      "| 54f68663|         u91|  jogo9|1612.55|3.23|2025-06-04 03:09:...|\n",
      "+---------+------------+-------+-------+----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = df_json.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "time.sleep(3)\n",
    "\n",
    "query.stop()\n",
    "query.awaitTermination() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1320d7f-b038-43eb-a634-c9fab8c9f105",
   "metadata": {},
   "source": [
    "### 2. Enrique√ßa o stream com os dados de apostadores do S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2737e55c-eb5d-441e-be86-280127cefcd1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/04 03:27:26 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-306ca8c9-d277-434d-9589-4cad9108e820. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/06/04 03:27:26 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "25/06/04 03:27:26 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+---------+------------+-------+-----+---+---------+---+----+-----+----+\n",
      "|aposta_id|apostador_id|jogo_id|valor|odd|timestamp| id|nome|email|pais|\n",
      "+---------+------------+-------+-----+---+---------+---+----+-----+----+\n",
      "+---------+------------+-------+-----+---+---------+---+----+-----+----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+---------+------------+-------+-------+----+--------------------+---+----------+--------------------+----+\n",
      "|aposta_id|apostador_id|jogo_id|  valor| odd|           timestamp| id|      nome|               email|pais|\n",
      "+---------+------------+-------+-------+----+--------------------+---+----------+--------------------+----+\n",
      "| 04185d59|         u58| jogo39|3899.83|3.62|2025-06-04 03:27:...|u58|Jogador 58|jogador58@exemplo...|  BR|\n",
      "| eb51fc23|         u56| jogo90|3440.01|8.93|2025-06-04 03:27:...|u56|Jogador 56|jogador56@exemplo...|  ES|\n",
      "| 2f43b8d8|         u41| jogo55|2196.31|5.54|2025-06-04 03:27:...|u41|Jogador 41|jogador41@exemplo...|  ES|\n",
      "| 60310221|         u66| jogo71|3513.35|1.88|2025-06-04 03:27:...|u66|Jogador 66|jogador66@exemplo...|  AR|\n",
      "| 97622c3d|         u96| jogo62|3024.61|1.94|2025-06-04 03:27:...|u96|Jogador 96|jogador96@exemplo...|  PT|\n",
      "| 57a2483d|         u58|  jogo5|3223.13|5.26|2025-06-04 03:27:...|u58|Jogador 58|jogador58@exemplo...|  BR|\n",
      "+---------+------------+-------+-------+----+--------------------+---+----------+--------------------+----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/04 03:27:29 ERROR WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 2, writer: ConsoleWriter[numRows=20, truncate=true]] is aborting.\n",
      "25/06/04 03:27:29 ERROR WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 2, writer: ConsoleWriter[numRows=20, truncate=true]] aborted.\n",
      "25/06/04 03:27:29 ERROR Utils: Aborting task\n",
      "org.apache.spark.TaskKilledException\n",
      "\tat org.apache.spark.TaskContextImpl.killTaskIfInterrupted(TaskContextImpl.scala:267)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:36)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:513)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "25/06/04 03:27:29 ERROR Utils: Aborting task\n",
      "org.apache.spark.TaskKilledException\n",
      "\tat org.apache.spark.TaskContextImpl.killTaskIfInterrupted(TaskContextImpl.scala:267)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:36)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:513)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "25/06/04 03:27:29 ERROR DataWritingSparkTask: Aborting commit for partition 1 (task 36, attempt 0, stage 19.0)\n",
      "25/06/04 03:27:29 ERROR DataWritingSparkTask: Aborted commit for partition 1 (task 36, attempt 0, stage 19.0)\n",
      "25/06/04 03:27:29 ERROR DataWritingSparkTask: Aborting commit for partition 0 (task 35, attempt 0, stage 19.0)\n",
      "25/06/04 03:27:29 ERROR DataWritingSparkTask: Aborted commit for partition 0 (task 35, attempt 0, stage 19.0)\n",
      "25/06/04 03:27:29 WARN TaskSetManager: Lost task 1.0 in stage 19.0 (TID 36) (8e63186473ca executor driver): TaskKilled (Stage cancelled: Job 19 cancelled part of cancelled job group 0a459430-1752-41e7-896a-264fa0cc493d)\n",
      "25/06/04 03:27:29 WARN TaskSetManager: Lost task 0.0 in stage 19.0 (TID 35) (8e63186473ca executor driver): TaskKilled (Stage cancelled: Job 19 cancelled part of cancelled job group 0a459430-1752-41e7-896a-264fa0cc493d)\n",
      "25/06/04 03:27:29 ERROR Utils: Aborting task\n",
      "org.apache.spark.TaskKilledException\n",
      "\tat org.apache.spark.TaskContextImpl.killTaskIfInterrupted(TaskContextImpl.scala:267)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:36)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:513)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "25/06/04 03:27:29 ERROR DataWritingSparkTask: Aborting commit for partition 2 (task 37, attempt 0, stage 19.0)\n",
      "25/06/04 03:27:29 ERROR DataWritingSparkTask: Aborted commit for partition 2 (task 37, attempt 0, stage 19.0)\n",
      "25/06/04 03:27:29 WARN TaskSetManager: Lost task 2.0 in stage 19.0 (TID 37) (8e63186473ca executor driver): TaskKilled (Stage cancelled: Job 19 cancelled part of cancelled job group 0a459430-1752-41e7-896a-264fa0cc493d)\n"
     ]
    }
   ],
   "source": [
    "df_apostadores = spark.read.csv('s3a://betalogs/apostadores.csv', header=True)\n",
    "df_join = df_json.join(df_apostadores, df_json.apostador_id == df_apostadores.id, \"inner\")\n",
    "\n",
    "query2 = df_join.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "time.sleep(3)\n",
    "\n",
    "query2.stop()\n",
    "query2.awaitTermination() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b45157e-a36b-414a-b48a-c3e922757bd2",
   "metadata": {},
   "source": [
    "### 3. Crie uma coluna booleana suspeita para apostas com valor > R$12.000 e odd > 15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a599365-fe34-454d-81c5-b671398fa1b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/04 03:09:25 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-1f2d281b-e394-4cca-8444-c79269f4a82e. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/06/04 03:09:25 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "25/06/04 03:09:25 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+---------+------------+-------+-----+---+---------+--------+\n",
      "|aposta_id|apostador_id|jogo_id|valor|odd|timestamp|suspeita|\n",
      "+---------+------------+-------+-----+---+---------+--------+\n",
      "+---------+------------+-------+-----+---+---------+--------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+---------+------------+-------+-------+----+--------------------+--------+\n",
      "|aposta_id|apostador_id|jogo_id|  valor| odd|           timestamp|suspeita|\n",
      "+---------+------------+-------+-------+----+--------------------+--------+\n",
      "| 05298021|         u67| jogo73| 947.27|7.04|2025-06-04 03:09:...|   false|\n",
      "| 2a4d098d|         u52| jogo50| 249.58|3.66|2025-06-04 03:09:...|   false|\n",
      "| fbe83e84|         u83| jogo93|1060.97|5.65|2025-06-04 03:09:...|   false|\n",
      "| 290c51b9|         u90| jogo46|2313.95| 3.7|2025-06-04 03:09:...|   false|\n",
      "| 3a479b2d|         u94| jogo36|1913.35| 9.8|2025-06-04 03:09:...|   false|\n",
      "| f3f693c3|         u30| jogo89|2955.34|6.03|2025-06-04 03:09:...|   false|\n",
      "| 13c0de38|         u61|jogo100|3717.88|6.77|2025-06-04 03:09:...|   false|\n",
      "| ba4992fd|         u48| jogo12|1553.81|2.29|2025-06-04 03:09:...|   false|\n",
      "| 39569069|         u73| jogo16|1164.39|9.36|2025-06-04 03:09:...|   false|\n",
      "| 9adc03e4|         u65| jogo98|1338.06|3.28|2025-06-04 03:09:...|   false|\n",
      "| 25bb3eb1|         u89| jogo99|3550.87|7.67|2025-06-04 03:09:...|   false|\n",
      "| 3ad3713f|         u28| jogo34|4185.57|9.63|2025-06-04 03:09:...|   false|\n",
      "| 551b03b1|          u4| jogo27| 849.36| 7.6|2025-06-04 03:09:...|   false|\n",
      "| 157f5e6c|         u82| jogo70|3472.81|5.73|2025-06-04 03:09:...|   false|\n",
      "| 864bdbae|         u73| jogo17| 2174.3|2.23|2025-06-04 03:09:...|   false|\n",
      "| 164158bd|         u94| jogo18|2556.07| 7.6|2025-06-04 03:09:...|   false|\n",
      "| ddffc80d|         u24| jogo56|3744.68|9.26|2025-06-04 03:09:...|   false|\n",
      "| 0cd5a4fa|         u19| jogo45|2122.84|2.11|2025-06-04 03:09:...|   false|\n",
      "| 57bcc243|         u87| jogo73|3033.02| 6.9|2025-06-04 03:09:...|   false|\n",
      "| bff62625|         u49| jogo99| 967.08|4.49|2025-06-04 03:09:...|   false|\n",
      "+---------+------------+-------+-------+----+--------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+---------+------------+-------+-------+----+--------------------+--------+\n",
      "|aposta_id|apostador_id|jogo_id|  valor| odd|           timestamp|suspeita|\n",
      "+---------+------------+-------+-------+----+--------------------+--------+\n",
      "| 5e19f6b1|         u22| jogo16|1334.86|1.43|2025-06-04 03:09:...|   false|\n",
      "| e3925e57|         u36| jogo63| 4027.2|2.14|2025-06-04 03:09:...|   false|\n",
      "| 103ecbb7|         u80| jogo59|3940.99|7.84|2025-06-04 03:09:...|   false|\n",
      "| f63eaac7|         u50|  jogo5| 160.62|4.03|2025-06-04 03:09:...|   false|\n",
      "| fd5ce00c|         u41| jogo65|4801.79|3.21|2025-06-04 03:09:...|   false|\n",
      "| dece1fb4|         u54| jogo62| 327.38|7.83|2025-06-04 03:09:...|   false|\n",
      "+---------+------------+-------+-------+----+--------------------+--------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 3\n",
      "-------------------------------------------\n",
      "+---------+------------+-------+-------+----+--------------------+--------+\n",
      "|aposta_id|apostador_id|jogo_id|  valor| odd|           timestamp|suspeita|\n",
      "+---------+------------+-------+-------+----+--------------------+--------+\n",
      "| 53790b86|          u5| jogo81| 224.65|8.19|2025-06-04 03:09:...|   false|\n",
      "| 99078fd4|         u98| jogo71| 1054.4|9.36|2025-06-04 03:09:...|   false|\n",
      "| 38449123|         u16| jogo55| 665.35|3.92|2025-06-04 03:09:...|   false|\n",
      "| a57e31c1|         u34| jogo59|2204.76|8.09|2025-06-04 03:09:...|   false|\n",
      "| 09fe3ede|         u59| jogo11|2784.13|7.58|2025-06-04 03:09:...|   false|\n",
      "| 20951300|         u99| jogo93| 3591.3|6.58|2025-06-04 03:09:...|   false|\n",
      "| 3ec0b2e5|         u68| jogo97|4359.49|7.51|2025-06-04 03:09:...|   false|\n",
      "| 717525c8|          u4| jogo75|2127.39|7.18|2025-06-04 03:09:...|   false|\n",
      "| affb5cee|         u10| jogo90|4098.74|3.43|2025-06-04 03:09:...|   false|\n",
      "| cb53b257|         u76| jogo84|4043.31|4.26|2025-06-04 03:09:...|   false|\n",
      "| 5daddc17|         u45| jogo44|3777.12|9.55|2025-06-04 03:09:...|   false|\n",
      "| ea4610f6|         u22| jogo41|3013.69|7.41|2025-06-04 03:09:...|   false|\n",
      "| 211d9956|         u40|  jogo2|4464.28|3.18|2025-06-04 03:09:...|   false|\n",
      "+---------+------------+-------+-------+----+--------------------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/04 03:09:28 ERROR WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 4, writer: ConsoleWriter[numRows=20, truncate=true]] is aborting.\n",
      "25/06/04 03:09:28 ERROR WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 4, writer: ConsoleWriter[numRows=20, truncate=true]] aborted.\n",
      "25/06/04 03:09:29 ERROR Utils: Aborting task\n",
      "org.apache.spark.TaskKilledException\n",
      "\tat org.apache.spark.TaskContextImpl.killTaskIfInterrupted(TaskContextImpl.scala:267)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:36)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "25/06/04 03:09:29 ERROR DataWritingSparkTask: Aborting commit for partition 0 (task 21, attempt 0, stage 9.0)\n",
      "25/06/04 03:09:29 ERROR DataWritingSparkTask: Aborted commit for partition 0 (task 21, attempt 0, stage 9.0)\n",
      "25/06/04 03:09:29 WARN TaskSetManager: Lost task 0.0 in stage 9.0 (TID 21) (8e63186473ca executor driver): TaskKilled (Stage cancelled: Job 9 cancelled part of cancelled job group ebd3f27f-f86a-4efd-a6b8-333a555e6662)\n"
     ]
    }
   ],
   "source": [
    "apostas_suspeitas = df_json.withColumn(\"suspeita\", expr(\"valor > 12000 AND odd > 15\"))\n",
    "\n",
    "query = apostas_suspeitas.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "time.sleep(3)\n",
    "\n",
    "query.stop()\n",
    "query.awaitTermination() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a28d73-43b2-4dd3-abbd-6510b2d22c27",
   "metadata": {},
   "source": [
    "### 4. A cada batch de 1 minuto, acumule:\n",
    "- ####  M√©dia de valor por pa√≠s.\n",
    "- ####  Contagem de apostas suspeitas por pa√≠s.\n",
    "- ####  Volume total de apostas.\n",
    "- ####  Contagem de apostas suspeitas por apostador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f30a13ac-df91-4808-96ff-abedce7eb14f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/04 22:57:13 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-87bc1b59-2384-489b-8118-7e302ab394bc. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/06/04 22:57:13 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "25/06/04 22:57:13 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+------+-------+-----------+\n",
      "|window|jogo_id|media_valor|\n",
      "+------+-------+-----------+\n",
      "+------+-------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+--------------------+-------+------------------+\n",
      "|              window|jogo_id|       media_valor|\n",
      "+--------------------+-------+------------------+\n",
      "|{2025-06-04 22:57...| jogo82|           1301.44|\n",
      "|{2025-06-04 22:57...| jogo43|            4022.9|\n",
      "|{2025-06-04 22:57...| jogo56|           1841.84|\n",
      "|{2025-06-04 22:57...| jogo46|           4385.14|\n",
      "|{2025-06-04 22:57...| jogo80|           2890.51|\n",
      "|{2025-06-04 22:57...| jogo39|           1275.68|\n",
      "|{2025-06-04 22:57...| jogo95|           3269.94|\n",
      "|{2025-06-04 22:57...| jogo13|           2508.33|\n",
      "|{2025-06-04 22:57...| jogo52|            3288.9|\n",
      "|{2025-06-04 22:57...| jogo51|            694.62|\n",
      "|{2025-06-04 22:57...| jogo34|           4297.94|\n",
      "|{2025-06-04 22:57...| jogo63|           1557.71|\n",
      "|{2025-06-04 22:57...| jogo77|           3167.69|\n",
      "|{2025-06-04 22:57...| jogo12|2918.7700000000004|\n",
      "|{2025-06-04 22:57...| jogo16|            1974.8|\n",
      "|{2025-06-04 22:57...| jogo85|            280.31|\n",
      "|{2025-06-04 22:57...| jogo35|           4956.19|\n",
      "|{2025-06-04 22:57...| jogo68|             501.1|\n",
      "|{2025-06-04 22:57...|  jogo9|           2193.31|\n",
      "|{2025-06-04 22:57...| jogo17|           1285.16|\n",
      "+--------------------+-------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/04 22:57:21 ERROR WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 2, writer: ConsoleWriter[numRows=20, truncate=true]] is aborting.\n",
      "25/06/04 22:57:21 ERROR WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 2, writer: ConsoleWriter[numRows=20, truncate=true]] aborted.\n",
      "25/06/04 22:57:21 WARN TaskSetManager: Lost task 14.0 in stage 159.0 (TID 15471) (8e63186473ca executor driver): TaskKilled (Stage cancelled: Job 79 cancelled part of cancelled job group d92daf3d-52b0-4845-ae92-acc2eb0a0ac0)\n",
      "25/06/04 22:57:21 WARN TaskSetManager: Lost task 18.0 in stage 159.0 (TID 15475) (8e63186473ca executor driver): TaskKilled (Stage cancelled: Job 79 cancelled part of cancelled job group d92daf3d-52b0-4845-ae92-acc2eb0a0ac0)\n",
      "25/06/04 22:57:21 WARN Shell: Interrupted while joining on: Thread[Thread-163288,5,main]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1313)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1381)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1042)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1002)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:900)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1212)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1306)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1288)\n",
      "\tat org.apache.hadoop.fs.FileUtil.readLink(FileUtil.java:212)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1113)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1590)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:453)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StreamingAggregationStateManagerBaseImpl.commit(StreamingAggregationStateManager.scala:90)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.$anonfun$doExecute$7(statefulOperators.scala:511)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.timeTakenMs(statefulOperators.scala:467)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.$anonfun$doExecute$5(statefulOperators.scala:511)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.package$StateStoreOps.$anonfun$mapPartitionsWithStateStore$1(package.scala:68)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStoreRDD.compute(StateStoreRDD.scala:127)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "25/06/04 22:57:21 WARN TaskSetManager: Lost task 19.0 in stage 159.0 (TID 15476) (8e63186473ca executor driver): TaskKilled (Stage cancelled: Job 79 cancelled part of cancelled job group d92daf3d-52b0-4845-ae92-acc2eb0a0ac0)\n",
      "25/06/04 22:57:21 WARN TaskSetManager: Lost task 17.0 in stage 159.0 (TID 15474) (8e63186473ca executor driver): TaskKilled (Stage cancelled: Job 79 cancelled part of cancelled job group d92daf3d-52b0-4845-ae92-acc2eb0a0ac0)\n",
      "25/06/04 22:57:21 WARN TaskSetManager: Lost task 20.0 in stage 159.0 (TID 15477) (8e63186473ca executor driver): TaskKilled (Stage cancelled: Job 79 cancelled part of cancelled job group d92daf3d-52b0-4845-ae92-acc2eb0a0ac0)\n",
      "25/06/04 22:57:21 WARN TaskSetManager: Lost task 15.0 in stage 159.0 (TID 15472) (8e63186473ca executor driver): TaskKilled (Stage cancelled: Job 79 cancelled part of cancelled job group d92daf3d-52b0-4845-ae92-acc2eb0a0ac0)\n",
      "25/06/04 22:57:21 WARN Shell: Interrupted while joining on: Thread[Thread-163289,5,main]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1313)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1381)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1042)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1002)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:900)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1212)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1306)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1288)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:324)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:294)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:439)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459)\n",
      "\tat org.apache.hadoop.fs.FileSystem.primitiveCreate(FileSystem.java:1305)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:102)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer.<init>(ChecksumFs.java:353)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.createInternal(ChecksumFs.java:400)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:626)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:701)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:697)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.create(FileContext.java:703)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createTempFile(CheckpointFileManager.scala:361)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:140)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:143)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createAtomic(CheckpointFileManager.scala:367)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream$lzycompute(HDFSBackedStateStoreProvider.scala:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream(HDFSBackedStateStoreProvider.scala:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream$lzycompute(HDFSBackedStateStoreProvider.scala:111)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream(HDFSBackedStateStoreProvider.scala:111)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StreamingAggregationStateManagerBaseImpl.commit(StreamingAggregationStateManager.scala:90)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.$anonfun$doExecute$7(statefulOperators.scala:511)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.timeTakenMs(statefulOperators.scala:467)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.$anonfun$doExecute$5(statefulOperators.scala:511)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.package$StateStoreOps.$anonfun$mapPartitionsWithStateStore$1(package.scala:68)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStoreRDD.compute(StateStoreRDD.scala:127)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "25/06/04 22:57:21 WARN TaskSetManager: Lost task 16.0 in stage 159.0 (TID 15473) (8e63186473ca executor driver): TaskKilled (Stage cancelled: Job 79 cancelled part of cancelled job group d92daf3d-52b0-4845-ae92-acc2eb0a0ac0)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import window, avg\n",
    "\n",
    "query = (\n",
    "    df_json\n",
    "        .withWatermark(\"timestamp\", \"2 minutes\")\n",
    "        .groupBy(window(\"timestamp\", \"1 minute\"),\"jogo_id\",\"pais\")\n",
    "        .agg(\n",
    "            avg(\"valor\").alias(\"media_valor\"),\n",
    "            .count(when(col(\"suspeita\") == \"sim\", True)).alias(\"contagem_suspeitas\"),\n",
    "            sum(\"valor\").alias(\"volume_total\")\n",
    "            )\n",
    "        .writeStream\n",
    "        .outputMode(\"complete\")\n",
    "        .format(\"console\")\n",
    "        .start()\n",
    "    \n",
    ")\n",
    "\n",
    "\n",
    "time.sleep(8)\n",
    "\n",
    "query.stop()\n",
    "query.awaitTermination() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2748eec1",
   "metadata": {},
   "source": [
    "### 5. Ap√≥s += 15 minutos, exiba o consolidado de todos os dados acumulados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7c43b6-4186-4a93-b347-12636e4b8006",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import window, avg\n",
    "\n",
    "query = (\n",
    "    df_json\n",
    "        .withWatermark(\"timestamp\", \"15 minutes\")\n",
    "        .groupBy(window(\"timestamp\", \"15 minutes\"),\"jogo_id\")\n",
    "        .agg(avg(\"valor\").alias(\"media_valor\"))\n",
    "        .writeStream\n",
    "        .outputMode(\"complete\")\n",
    "        .format(\"console\")\n",
    "        .start()\n",
    "    \n",
    ")\n",
    "\n",
    "query.awaitTermination() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
