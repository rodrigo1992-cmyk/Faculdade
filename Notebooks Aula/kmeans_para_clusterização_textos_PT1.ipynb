{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "I6-KfyG718oV"
      },
      "outputs": [],
      "source": [
        "#Criando um dataset fictício\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "cat = ['alt.atheism', 'talk.religion.misc', 'comp.graphics',\n",
        "       'sci.space']\n",
        "\n",
        "dataset = fetch_20newsgroups(subset='all', categories=cat)\n",
        "labels = dataset.target\n",
        "label_names = dataset.target_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "mZRQbAOfAhDo"
      },
      "outputs": [],
      "source": [
        "#Fazendo Regex para limpar o texto\n",
        "\n",
        "import numpy as np\n",
        "# from nltk.corpus import names\n",
        "import re\n",
        "import string\n",
        "\n",
        "def preproc(data):\n",
        "  url_re = re.compile(r'(?:http|ftp|https)://(?:[\\w_-]+(?:(?:\\.[\\w_-]+)+))(?:[\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?')\n",
        "  email_re = re.compile('(?:[a-z0-9!#$%&\\'*+/=?^_`{|}~-]+(?:\\.[a-z0-9!#$%&\\'*+/=?^_`{|}~-]+)*|\"(?:[\\x01-\\x08\\x0b\\x0c\\x0e-\\x1f\\x21\\x23-\\x5b\\x5d-\\x7f]|\\\\[\\x01-\\x09\\x0b\\x0c\\x0e-\\x7f])*\")@(?:(?:[a-z0-9](?:[a-z0-9-]*[a-z0-9])?\\.)+[a-z0-9](?:[a-z0-9-]*[a-z0-9])?|\\[(?:(?:(2(5[0-5]|[0-4][0-9])|1[0-9][0-9]|[1-9]?[0-9]))\\.){3}(?:(2(5[0-5]|[0-4][0-9])|1[0-9][0-9]|[1-9]?[0-9])|[a-z0-9-]*[a-z0-9]:(?:[\\x01-\\x08\\x0b\\x0c\\x0e-\\x1f\\x21-\\x5a\\x53-\\x7f]|\\\\[\\x01-\\x09\\x0b\\x0c\\x0e-\\x7f])+)\\])')\n",
        "  preproc_data = []\n",
        "  for doc in data:\n",
        "    # Remover cabeçalho de Email\n",
        "    doc = re.sub(r'(From:\\s+[^\\n]+\\n)', '', doc)\n",
        "    doc = re.sub(r'(Subject:[^\\n]+\\n)', '', doc)\n",
        "    doc = re.sub(r'(([\\sA-Za-z0-9\\-]+)?[A|a]rchive-name:[^\\n]+\\n)', '', doc)\n",
        "    doc = re.sub(r'(Last-modified:[^\\n]+\\n)', '', doc)\n",
        "    doc = re.sub(r'(Version:[^\\n]+\\n)', '', doc)\n",
        "\n",
        "    doc = doc.lower()\n",
        "    doc = re.sub(url_re, '', doc)\n",
        "    doc = re.sub(email_re, '', doc)\n",
        "    doc = re.sub(f'[{re.escape(string.punctuation)}]', '', doc)\n",
        "    doc = re.sub(r'(\\d+)', ' ', doc)\n",
        "    preproc_data.append(doc)\n",
        "  return np.array(preproc_data)\n",
        "\n",
        "\n",
        "#Leematizando o texto\n",
        "\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "def get_lemmatized_data(data: list) -> np.array:\n",
        "  data_proc = []\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  for doc in data:\n",
        "    doc = doc.lower()\n",
        "    doc_lem = ' '.join(lemmatizer.lemmatize(word) for word in doc.split())\n",
        "    data_proc.append(doc_lem)\n",
        "  return np.array(data_proc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "8AdaqtcRAhnD",
        "outputId": "e8e70a52-b182-4f8b-979c-b11107c25eb4"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'line organization walla walla college line in article sn mozumder writes date wed apr gmt in article tammy r healy writes bobby i would like to take the liberty to quote from a christian writer named ellen g white i hope that what she said will help you to edit your remark in this group in the future do not set yourself a a standard do not make your opinion your view of duty your interpretation of scripture a criterion for others and in your heart condemn them if they do not come up to your ideal thought fromthe mount of blessing p i hope quoting this doesnt make the atheist gag but i think ellen white put it better than i could tammy point peace bobby mozumder my point is that you set up your view a the only way to believe saying that all eveil in this world is caused by atheism is ridiculous and counterproductive to dialogue in this newsgroups i see in your post a spirit of condemnation of the atheist in this newsgroup bacause they don t believe exactly a you do if youre here to try to convert the atheist here youre failing miserably who want to be in position of constantly defending themselves agaist insulting attack like you seem to like to do im sorry youre so blind that you didnt get the messgae in the quote everyone else ha seemed to tammy'"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data_proc = preproc(dataset.data)\n",
        "daat_lem = get_lemmatized_data(data_proc)\n",
        "data_lem[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "9Ca6xGzTF1qs"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "bow = CountVectorizer(max_df=0.5, min_df=2, stop_words='english')\n",
        "vect = bow.fit_transform(data_lem)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "id": "C5l--U_gGnlm",
        "outputId": "a065216a-ab5c-41f8-f8f6-1d519db72fa4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KMeans(n_clusters=4, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KMeans</label><div class=\"sk-toggleable__content\"><pre>KMeans(n_clusters=4, random_state=42)</pre></div></div></div></div></div>"
            ],
            "text/plain": [
              "KMeans(n_clusters=4, random_state=42)"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "K = 4\n",
        "kmeans = KMeans(n_clusters=K, random_state=42)\n",
        "kmeans.fit(vect)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WBbwYF9iHVwP",
        "outputId": "17484102-7145-4e99-e9d9-686196ebdf6e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Counter({1: 3376, 0: 7, 2: 3, 3: 1})"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from collections import Counter\n",
        "clusters = kmeans.labels_\n",
        "Counter(clusters)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mtgTsDBuHnye",
        "outputId": "0628b53e-fdd3-4bac-df81-93d335f5d8ca"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['aa', 'aah', 'aap', ..., 'zyda', 'zyxel', 'zz'], dtype=object)"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bow.get_feature_names_out()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KT9iKAJJH7JF",
        "outputId": "620cdc97-7252-49fe-ed7e-a6977f6962a6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Counter({0: 575, 1: 942, 3: 1739, 2: 131})"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf = TfidfVectorizer(max_df=0.5, min_df=2, stop_words='english')\n",
        "\n",
        "vect_tfidf = tfidf.fit_transform(data_lem)\n",
        "kmeans.fit(vect_tfidf)\n",
        "clusters = kmeans.labels_\n",
        "Counter(clusters)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iiCioegfI62m",
        "outputId": "88180977-f805-4ecb-c123-a7f764675256"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{0: array([0, 0, 3, 0, 0, 0, 0, 0, 3, 3, 3, 0, 3, 0, 0, 3, 3, 0, 3, 3, 0, 3,\n",
              "        0, 3, 3, 3, 0, 0, 0, 0, 0, 0, 3, 0, 0, 3, 0, 0, 3, 0, 3, 3, 0, 0,\n",
              "        0, 0, 3, 3, 0, 0, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0,\n",
              "        0, 3, 0, 0, 0, 3, 3, 0, 3, 0, 3, 0, 0, 3, 3, 0, 0, 3, 0, 0, 3, 0,\n",
              "        3, 0, 3, 3, 0, 0, 0, 3, 0, 3, 0, 0, 3, 3, 0, 3, 0, 3, 0, 3, 3, 0,\n",
              "        3, 0, 3, 3, 0, 0, 3, 3, 0, 0, 0, 3, 0, 0, 3, 3, 0, 3, 3, 0, 0, 0,\n",
              "        3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 3, 0, 0, 3, 3, 3,\n",
              "        0, 0, 0, 0, 3, 0, 3, 3, 0, 3, 0, 3, 0, 0, 0, 3, 3, 3, 0, 0, 3, 0,\n",
              "        0, 0, 0, 0, 3, 3, 0, 0, 3, 0, 0, 3, 3, 3, 3, 3, 0, 0, 0, 0, 3, 3,\n",
              "        3, 3, 3, 0, 3, 0, 0, 0, 0, 3, 0, 0, 3, 0, 0, 0, 0, 0, 3, 0, 0, 0,\n",
              "        0, 0, 0, 3, 3, 3, 0, 3, 0, 0, 0, 0, 3, 0, 3, 0, 3, 0, 3, 3, 3, 0,\n",
              "        0, 3, 0, 3, 3, 3, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 3, 3, 3, 3, 0,\n",
              "        0, 3, 0, 3, 3, 0, 3, 0, 0, 3, 0, 3, 0, 0, 0, 0, 0, 3, 3, 0, 3, 0,\n",
              "        3, 0, 0, 0, 3, 0, 3, 0, 0, 3, 0, 3, 0, 3, 3, 0, 0, 0, 1, 0, 0, 3,\n",
              "        0, 0, 0, 0, 3, 0, 3, 0, 0, 0, 3, 3, 0, 3, 0, 3, 0, 0, 3, 0, 0, 0,\n",
              "        3, 3, 3, 3, 3, 3, 3, 0, 3, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 3, 0, 0,\n",
              "        3, 0, 3, 3, 0, 3, 0, 0, 3, 0, 0, 3, 0, 0, 3, 3, 3, 0, 0, 0, 3, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 3, 0, 3, 3, 0, 0, 3, 0, 0, 0, 0, 0, 3, 0, 0,\n",
              "        0, 3, 3, 0, 3, 3, 3, 0, 3, 0, 0, 3, 3, 0, 3, 0, 3, 0, 0, 0, 3, 0,\n",
              "        0, 0, 3, 3, 0, 0, 0, 0, 0, 0, 3, 0, 3, 3, 0, 3, 3, 3, 0, 0, 0, 0,\n",
              "        0, 3, 3, 3, 3, 0, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 0, 0, 0, 0,\n",
              "        0, 0, 3, 0, 0, 0, 0, 0, 3, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3,\n",
              "        3, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 0, 0, 0, 3, 0, 0, 3, 3, 0, 3,\n",
              "        0, 0, 0, 0, 3, 3, 0, 0, 3, 0, 3, 3, 0, 0, 0, 3, 0, 3, 0, 3, 3, 3,\n",
              "        0, 0, 0, 3, 0, 0, 3, 0, 0, 3, 0, 0, 0, 0, 3, 0, 3, 0, 0, 0, 0, 0,\n",
              "        3, 0, 0, 3, 0, 3, 0, 3, 3, 3, 3, 0, 3, 3, 0, 3, 0, 0, 0, 0, 3, 3,\n",
              "        3, 3, 0]),\n",
              " 1: array([1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        2, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1,\n",
              "        1, 2, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 0, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 2, 1, 1, 3, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 3, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 3, 1, 1, 1, 1, 1, 2, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2,\n",
              "        1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2,\n",
              "        1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 2, 1, 1,\n",
              "        1, 0, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 2, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1,\n",
              "        2, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 0, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 2, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1,\n",
              "        1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1,\n",
              "        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1]),\n",
              " 2: array([0, 0, 0, 3, 0, 0, 0, 3, 3, 0, 0, 3, 0, 0, 0, 3, 0, 0, 0, 3, 3, 0,\n",
              "        3, 3, 0, 0, 3, 3, 3, 3, 0, 3, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 3,\n",
              "        0, 3, 0, 0, 0, 3, 0, 0, 0, 3, 0, 3, 3, 0, 3, 0, 3, 0, 3, 0, 3, 3,\n",
              "        0, 0, 0, 3, 0, 3, 0, 3, 3, 0, 3, 3, 0, 0, 0, 3, 3, 0, 0, 3, 0, 3,\n",
              "        3, 0, 0, 3, 0, 3, 0, 0, 3, 3, 3, 0, 0, 3, 0, 3, 0, 0, 3, 0, 0, 0,\n",
              "        0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 3, 3, 0, 0, 3, 3, 3, 0, 3, 0, 0]),\n",
              " 3: array([3, 2, 2, ..., 0, 2, 2])}"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cluster_label = {i: labels[np.where(clusters == i)] for i in range(K)}\n",
        "cluster_label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FGJ7DXo-K5lg",
        "outputId": "a265bd99-1198-4438-e17a-bbb7e36b58b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cluster 0: 575 amostras\n",
            "=========================\n",
            "alt.atheism: 342 amostras\n",
            "talk.religion.misc: 232 amostras\n",
            "comp.graphics: 1 amostras\n",
            "=========================\n",
            "Top 10 palavras:\n",
            "bible \n",
            "believe \n",
            "atheist \n",
            "say \n",
            "people \n",
            "religion \n",
            "wa \n",
            "christian \n",
            "jesus \n",
            "god \n",
            "=========================\n",
            "\n",
            "\n",
            "Cluster 1: 942 amostras\n",
            "=========================\n",
            "comp.graphics: 856 amostras\n",
            "sci.space: 70 amostras\n",
            "alt.atheism: 9 amostras\n",
            "talk.religion.misc: 7 amostras\n",
            "=========================\n",
            "Top 10 palavras:\n",
            "nntppostinghost \n",
            "email \n",
            "bit \n",
            "format \n",
            "program \n",
            "university \n",
            "thanks \n",
            "graphic \n",
            "file \n",
            "image \n",
            "=========================\n",
            "\n",
            "\n",
            "Cluster 2: 131 amostras\n",
            "=========================\n",
            "alt.atheism: 79 amostras\n",
            "talk.religion.misc: 52 amostras\n",
            "=========================\n",
            "Top 10 palavras:\n",
            "dont \n",
            "odwyer \n",
            "keith \n",
            "people \n",
            "immoral \n",
            "think \n",
            "value \n",
            "morality \n",
            "moral \n",
            "objective \n",
            "=========================\n",
            "\n",
            "\n",
            "Cluster 3: 1739 amostras\n",
            "=========================\n",
            "sci.space: 917 amostras\n",
            "alt.atheism: 369 amostras\n",
            "talk.religion.misc: 337 amostras\n",
            "comp.graphics: 116 amostras\n",
            "=========================\n",
            "Top 10 palavras:\n",
            "think \n",
            "ha \n",
            "dont \n",
            "like \n",
            "university \n",
            "just \n",
            "nntppostinghost \n",
            "article \n",
            "space \n",
            "wa \n",
            "=========================\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "palavras = tfidf.get_feature_names_out()\n",
        "centroids = kmeans.cluster_centers_\n",
        "\n",
        "for cluster, index_list in cluster_label.items():\n",
        "  counter = Counter(cluster_label[cluster])\n",
        "  print(f'Cluster {cluster}: {len(index_list)} amostras')\n",
        "  print(\"=========================\")\n",
        "  for label_index, count in sorted(counter.items(), key=lambda x: x[1],\n",
        "                                   reverse=True):\n",
        "    print(f'{label_names[label_index]}: {count} amostras')\n",
        "  print(\"=========================\")\n",
        "  print(\"Top 10 palavras:\")\n",
        "  for i in centroids[cluster].argsort()[-10:]:\n",
        "    print(f\"{palavras[i]} \")\n",
        "  print(\"=========================\")\n",
        "  print(\"\\n\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
