{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercício 1:\n",
    "#### Utilize XPath absoluto com a biblioteca lxml para selecionar:\n",
    "- O nome do diretor do segundo filme listado;\n",
    "- O id do penúltimo filme listado, independentemente da quantidade de filmes na lista. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lxml in c:\\users\\rodrigopintomesquita\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (5.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.1 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diretor do diretor segundo filme listado: Francis Ford Coppola\n",
      "ID do penúltimo filme listado: 102\n"
     ]
    }
   ],
   "source": [
    "from lxml import etree\n",
    "\n",
    "tree = etree.parse(r'C:\\Users\\RodrigoPintoMesquita\\Documents\\GitHub\\Faculdade\\Notebooks Trabalhos 2024H2\\DR2_TP3\\Dados\\TP3.xml', parser=etree.XMLParser())\n",
    "root = tree.getroot()\n",
    "\n",
    "diretor = root.xpath('/library/movies/movie[2]/director/text()')\n",
    "penultimo_id = root.xpath('/library/movies/movie[last()-1]/@id')\n",
    "\n",
    "print(f'Diretor do diretor segundo filme listado: {diretor[0]}')\n",
    "print(f'ID do penúltimo filme listado: {penultimo_id[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercício 2:\n",
    "#### Utilize XPath relativo com a biblioteca lxml para selecionar:\n",
    "- Os títulos dos livros ou filmes da categoria fantasia;\n",
    "- Os nomes (siglas) das moedas utilizadas nos preços que não são dólares americanos (USD);\n",
    "- Os preços com valor numérico maior que 15 (independentemente da moeda);\n",
    "- Todos os títulos que começam com \"The\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Livros ou filmes da categoria fantasia: [\"Harry Potter and the Philosopher's Stone\", 'The Hobbit']\n",
      "Moedas que não são dólares americanos: ['NZD', 'GBP', 'BRL']\n",
      "Preços com valor numérico maior que 15: ['19.99', '15.99', '19.99']\n",
      "Títulos que começam com \"The\": ['The Hobbit', 'The Godfather']\n"
     ]
    }
   ],
   "source": [
    "resp1 = root.xpath('.//*[@category=\"fantasy\"]/title/text()')\n",
    "resp2 = root.xpath('.//price[@currency!=\"USD\"]/attribute::currency')\n",
    "resp3 = root.xpath('.//price[number(text()) > 15]/text()')\n",
    "resp4 = root.xpath('.//title[starts-with(text(), \"The\")]/text()')\n",
    "\n",
    "print(f'Livros ou filmes da categoria fantasia: {resp1}')\n",
    "print(f'Moedas que não são dólares americanos: {resp2}')\n",
    "print(f'Preços com valor numérico maior que 15: {resp3}')\n",
    "print(f'Títulos que começam com \"The\": {resp4}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercício 3:\n",
    "#### Utilize (apenas) seletores de CSS com lxml para selecionar:\n",
    "- Todos os nomes de países dos atletas;\n",
    "- Todos os nomes de atletas que possuem a letra J (maiúscula);\n",
    "- Todos os nomes de atletas dos EUA (USA) e da Rússia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting cssselect\n",
      "  Downloading cssselect-1.2.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
      "Downloading cssselect-1.2.0-py2.py3-none-any.whl (18 kB)\n",
      "Installing collected packages: cssselect\n",
      "Successfully installed cssselect-1.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.1 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install cssselect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USA\n",
      "Canada\n",
      "China\n",
      "Mexico\n",
      "Russia\n"
     ]
    }
   ],
   "source": [
    "from lxml.cssselect import CSSSelector\n",
    "\n",
    "fpath = r'C:\\Users\\RodrigoPintoMesquita\\Documents\\GitHub\\Faculdade\\Notebooks Trabalhos 2024H2\\DR2_TP3\\Dados\\TP3.html'\n",
    "doc = etree.parse(fpath, parser=etree.HTMLParser()).getroot()\n",
    "doc_paises = doc.cssselect('p.country')\n",
    "\n",
    "#Resposta 1 - Todos os nomes de países dos atletas\n",
    "for x in doc_paises:\n",
    "    print(x.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "John Doe\n",
      "Jane Smith\n"
     ]
    }
   ],
   "source": [
    "#Resposta 2- Todos os nomes de atletas que possuem a letra J (maiúscula)\n",
    "\n",
    "doc_nomes = tree.cssselect('.name')\n",
    "for nome in doc_nomes:\n",
    "    if \"J\" in nome.text:\n",
    "        print(nome.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['John Doe', 'Olga Ivanova']\n"
     ]
    }
   ],
   "source": [
    "#Resposta 3 - Todos os nomes de atletas dos EUA (USA) e da Rússia.\n",
    "\n",
    "resposta = []\n",
    "for atleta in doc.cssselect('.athlete'):\n",
    "    pais = atleta.cssselect('.country')[0].text\n",
    "    if pais in ['USA', 'Russia']:\n",
    "        nome = atleta.cssselect('.name')[0].text\n",
    "        resposta.append(nome)\n",
    "\n",
    "print(resposta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercício 4:\n",
    "\n",
    "#### Utilize seletores de CSS com BeautifulSoup e algum tratamento em Python que seja necessário para alocar as informações de cada atleta do arquivo HTML em um dataframe do Pandas a ser salvo em arquivo CSV e então responda:\n",
    "\n",
    "- A média de idades dos atletas;\n",
    "- O atleta ganhador do Speed Test;\n",
    "- O atleta ganhador do Endurance Test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercício 5: \n",
    "\n",
    "#### Acesse a URL https://www.scrapethissite.com/pages/simple/ e obtenha os dados de todos os países, salvando em um dataframe (sem utilizar o comando `pd.read_html`) com as seguintes colunas:\n",
    "\n",
    "- Country;\n",
    "- Capital;\n",
    "- Population;\n",
    "- Area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercício 6: \n",
    "\n",
    "#### Escolha uma notícia qualquer do site https://www.romanews.com.br/ e obtenha o HTML da página via requisição utilizando a biblioteca que preferir (`urllib`, `requests`, etc.), salvando o arquivo no seu diretório. Então, a partir do arquivo HTML da notícia salva, construa códigos em Python utilizando `BeautifulSoup` para obter e salvar em um arquivo JSON, junto à URL da notícia e ao datetime do momento da requisição da página (os objetos datetime devem ter a informação da timezone e ser colocados em isoformat no momento de gerar o JSON):\n",
    "\n",
    "- O objeto datetime da data e hora da publicação da notícia (dica: é sempre bom checar as tags `meta` dentro do `head` do arquivo.);\n",
    "- O título da notícia;\n",
    "- O corpo do texto da notícia;\n",
    "- O subtítulo da notícia (se houver);\n",
    "- O autor ou autores da notícia (se houver)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercício 7: \n",
    "\n",
    "#### Repita a questão anterior utilizando um site de notícias de sua escolha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercício 8: \n",
    "#### Examine o site https://difusoranews.com/ e descubra o padrão de URL para paginação que ele aceita (dica: considere tentativa-e-erro). Então, utilize-o para obter uma lista de links de notícias requisitando as cinco primeiras páginas e raspando os links através de um único seletor de CSS aplicado via `BeautifulSoup`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercício 9:\n",
    "\n",
    "#### Repita a questão anterior utilizando um site de notícias de sua escolha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercício 10: \n",
    "\n",
    "#### Consulte o arquivo `robots.txt` do site https://tecnoblog.net/ para encontrar o link do `sitemap_index` e obtenha a partir dele a lista de todos os sitemaps de notícias (filtrando apenas os de notícias)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercício 11: \n",
    "\n",
    "#### Escolha um dos sites de exemplo em https://toscrape.com/ para criar um projeto no Scrapy que obtenha os principais dados disponíveis, salvando em CSV ou JSON conforme a conveniência em relação aos dados do site escolhido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercício 12: \n",
    "\n",
    "#### Escolha um dos sites utilizados nas questões 6, 7, 8 e 9 para montar um projeto no Scrapy que abarque tanto o Crawling quanto o Scraping, a fim de rodá-lo tal como na questão anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
