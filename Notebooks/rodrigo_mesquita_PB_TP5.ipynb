{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Rodrigo_Mesquita_PB_TP5**\n",
    "## **Projeto de Bloco: Inteligência Artificial e Machine Learning [24E1_5]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.Criação das features: Computar o Term Frequency-Inverse Document Frequency (TF-IDF) para representar a importância das palavras em um conjunto de documentos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install nltk\n",
    "#%pip install yellowbrick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from yellowbrick.cluster import SilhouetteVisualizer\n",
    "from collections import Counter\n",
    "from sklearn.metrics import silhouette_score\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['font.sans-serif'] = 'DejaVu Sans'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantidade de Documentos: 1391\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A Beginner’s Guide to Word Embedding with Gens...</td>\n",
       "      <td>1. Introduction of Word2vec\\n\\nWord2vec is one...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hands-on Graph Neural Networks with PyTorch &amp; ...</td>\n",
       "      <td>In my last article, I introduced the concept o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How to Use ggplot2 in Python</td>\n",
       "      <td>Introduction\\n\\nThanks to its strict implement...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Databricks: How to Save Data Frames as CSV Fil...</td>\n",
       "      <td>Photo credit to Mika Baumeister from Unsplash\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A Step-by-Step Implementation of Gradient Desc...</td>\n",
       "      <td>A Step-by-Step Implementation of Gradient Desc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  \\\n",
       "0  A Beginner’s Guide to Word Embedding with Gens...   \n",
       "1  Hands-on Graph Neural Networks with PyTorch & ...   \n",
       "2                       How to Use ggplot2 in Python   \n",
       "3  Databricks: How to Save Data Frames as CSV Fil...   \n",
       "4  A Step-by-Step Implementation of Gradient Desc...   \n",
       "\n",
       "                                                Text  \n",
       "0  1. Introduction of Word2vec\\n\\nWord2vec is one...  \n",
       "1  In my last article, I introduced the concept o...  \n",
       "2  Introduction\\n\\nThanks to its strict implement...  \n",
       "3  Photo credit to Mika Baumeister from Unsplash\\...  \n",
       "4  A Step-by-Step Implementation of Gradient Desc...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/workspaces/Faculdade/Datasets/medium.csv')\n",
    "X = df['Text'].str.lower()\n",
    "\n",
    "print(f'Quantidade de Documentos: {len(df)}')\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aplicando Regex para manter apenas palavras com 3 ou mais caracteres, retirando pontuações e números\n",
    "import re\n",
    "pattern = re.compile(r\"\\b[a-zA-Z]{3,}\\b\")\n",
    "X_reg = X.apply(lambda x: (' '.join(pattern.findall(str(x)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Leematizando o texto\n",
    "data_lem = []\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for doc in X_reg:\n",
    "  doc = doc.lower()\n",
    "  doc_lem = ' '.join(lemmatizer.lemmatize(word) for word in doc.split())\n",
    "  data_lem.append(doc_lem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similarity    0.424331\n",
      "gensim        0.328443\n",
      "list          0.262889\n",
      "embedding     0.204936\n",
      "cosine        0.188221\n",
      "sne           0.177180\n",
      "item          0.173900\n",
      "vehicle       0.158402\n",
      "gram          0.150966\n",
      "van           0.141744\n",
      "Name: 0, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(max_df=0.2, min_df=5, stop_words='english')\n",
    "vect = tfidf.fit_transform(data_lem)\n",
    "\n",
    "# Convertendo a matriz TF-IDF para um DataFrame\n",
    "df_tfidf = pd.DataFrame(vect.toarray(), columns= tfidf.get_feature_names_out())\n",
    "\n",
    "# Exibindo as palavras com maior pontuação:\n",
    "print(df_tfidf.loc[0].sort_values(ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.Modelagem de Tópicos com LDA: Aplicar o algoritmo LDA para identificar tópicos prevalentes nos dados. A seleção do número de tópicos será baseada em métricas de coerência para garantir a relevância e a distinção entre os tópicos identificados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install gensim\n",
    "#%pip install scipy==1.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/codespace/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from gensim.models import CoherenceModel\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim import corpora, models\n",
    "import pandas as pd\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de tópicos: 2, Coerência: 0.2717\n",
      "Número de tópicos: 3, Coerência: 0.2699\n",
      "Número de tópicos: 4, Coerência: 0.2730\n",
      "Número de tópicos: 5, Coerência: 0.2821\n",
      "Número de tópicos: 6, Coerência: 0.2832\n",
      "Número de tópicos: 8, Coerência: 0.2731\n",
      "Número de tópicos: 10, Coerência: 0.2700\n",
      "Número de tópicos: 12, Coerência: 0.2719\n",
      "Número de tópicos: 16, Coerência: 0.2757\n",
      "Número de tópicos: 20, Coerência: 0.2723\n"
     ]
    }
   ],
   "source": [
    "# Criando o dicionário e o corpus\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "tokens = [[word for word in doc if word not in stop_words] for doc in [word_tokenize(doc.lower()) for doc in data_lem]]\n",
    "\n",
    "dicionario = corpora.Dictionary(tokens)\n",
    "\n",
    "corpus = [dicionario.doc2bow(doc) for doc in tokens]\n",
    "\n",
    "# Iterar sobre diferentes valores de num_topics\n",
    "topics_range = [2,3,4,5,6,8,10,12,16,20]\n",
    "coherence_values = []\n",
    "\n",
    "for num_topics in topics_range:\n",
    "    # Construir o modelo LDA\n",
    "    lda_model = models.LdaMulticore(corpus=corpus, id2word=dicionario,num_topics=num_topics)\n",
    "\n",
    "    # Calcular a coerência\n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts=tokens, dictionary=dicionario, coherence='c_v')\n",
    "    coherence_lda = coherence_model_lda.get_coherence()\n",
    "    coherence_values.append(coherence_lda)\n",
    "\n",
    "    print(f\"Número de tópicos: {num_topics}, Coerência: {coherence_lda:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de tópicos: 2, Coerência: 0.2732\n",
      "Número de tópicos: 3, Coerência: 0.2779\n",
      "Número de tópicos: 4, Coerência: 0.2587\n",
      "Número de tópicos: 5, Coerência: 0.2675\n",
      "Número de tópicos: 6, Coerência: 0.2683\n",
      "Número de tópicos: 8, Coerência: 0.2667\n",
      "Número de tópicos: 10, Coerência: 0.2728\n",
      "Número de tópicos: 12, Coerência: 0.2620\n",
      "Número de tópicos: 16, Coerência: 0.2673\n",
      "Número de tópicos: 20, Coerência: 0.2692\n"
     ]
    }
   ],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "\n",
    "tokens = [[word for word in doc if word not in stop_words] for doc in [word_tokenize(doc.lower()) for doc in data_lem]]\n",
    "\n",
    "dicionario = corpora.Dictionary(tokens)\n",
    "dicionario.filter_extremes(no_below=5, no_above=0.5)\n",
    "\n",
    "corpus = [dicionario.doc2bow(doc) for doc in tokens]\n",
    "\n",
    "# Iterar sobre diferentes valores de num_topics\n",
    "topics_range = [2, 3, 4, 5, 6, 8, 10, 12, 16, 20]\n",
    "coherence_values = []\n",
    "\n",
    "for num_topics in topics_range:\n",
    "    # Construir o modelo LDA\n",
    "    lda_model = models.LdaMulticore(corpus=corpus, id2word=dicionario, num_topics=num_topics)\n",
    "\n",
    "    # Calcular a coerência\n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts=tokens, dictionary=dicionario, coherence='c_v')\n",
    "    coherence_lda = coherence_model_lda.get_coherence()\n",
    "    coherence_values.append(coherence_lda)\n",
    "\n",
    "    print(f\"Número de tópicos: {num_topics}, Coerência: {coherence_lda:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.Classificação de Textos: Desenvolver modelos de classificação para categorizar os textos com base nos tópicos identificados. Você pode escolher qualquer modelo aprendido ao longo do curso e deve escolher o melhor modelo usando as técnicas aprendidas, como busca de hiperparâmetros e validação cruzada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.Avaliação de Desempenho: O desempenho dos modelos de classificação será avaliado utilizando métricas como precisão, recall, F1-score e AUC-ROC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.Visualização com t-SNE: Aplicar a técnica de t-SNE nos dados textuais vetorizados para reduzir a dimensionalidade e visualizar os agrupamentos de documentos de maneira intuitiva, facilitando a identificação de padrões e outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.Interpretação de Modelos com LIME, SHAP e Force-Plot: Utilizar SHAP para explicar as previsões individuais, identificando a contribuição de cada feature para a decisão do modelo. O force-plot será usado para visualizar essas contribuições de maneira agregada, oferecendo insights sobre a lógica de decisão do modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.Análise dos resultados: Enumere as conclusões que podem ser tomadas a partir dos resultados obtidos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.Ambiente Recomendado: A solução pode ser desenvolvida em qualquer ambiente de sua escolha, embora seja recomendado o uso do Google Colab Notebook. Lembre-se de comentar seu código para facilitar a compreensão da análise e do procedimento adotado."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
